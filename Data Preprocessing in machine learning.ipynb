{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING IN MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format.<br>\n",
    "<br>\n",
    "The type of cleaning and engineering strategies used usually depend on the business problem and type of target variable, since this will influence the algorithm and data preparation requirements.\n",
    "<br>\n",
    "\n",
    "You almost always need to preprocess your data. It is a required step.<br>\n",
    "<br>\n",
    "A difficulty is that different algorithms make different assumptions about your data and may require different transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms can deliver better results without the preprocessing.<br>\n",
    "<br>\n",
    "Generally, I would recommend creating many different views and transforms of your data, then exercise a handful of algorithms on each view of your dataset. This will help you to flush out which data transforms might be better at exposing the structure of your problem in general.\n",
    "<br>\n",
    "<br>\n",
    "#### Why Preprocessing ?<br>\n",
    "Real world data are generally<br>\n",
    "<br>\n",
    "__Incomplete__: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data<br>\n",
    "__Noisy__: containing errors or outliers<br>\n",
    "__Inconsistent__: containing discrepancies in codes or names<br>\n",
    "\n",
    "#### Tasks in data preprocessing<br>\n",
    "__Data cleaning__: fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies.<br>\n",
    "__Data integration__: using multiple databases, data cubes, or files.<br>\n",
    "__Data transformation__: normalization and aggregation.<br>\n",
    "__Data reduction__: reducing the volume but producing the same or similar analytical results.<br>\n",
    "__Data discretization__: part of data reduction, replacing numerical attributes with nominal ones.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mlflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "\n",
    "Feature engineering is an essential part of building any intelligent system. Even though you have a lot of newer methodologies coming in like deep learning and meta-heuristics which aid in automated machine learning, each problem is domain specific and better features (suited to the problem) is often the deciding factor of the performance of your system. Feature Engineering is an art as well as a science and this is the reason Data Scientists often spend 70% of their time in the data preparation phase before modeling. Let’s look at a few quotes relevant to feature engineering from several renowned people in the world of Data Science.\n",
    "\n",
    "<br>\n",
    "“Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.” <br>\n",
    "— Prof. Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET AND LIBRARY\n",
    "\n",
    "We will be using Loan Prediction dataset which is available in my github. The most popular framework for machine learning in python is __scikit-learn__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# training data\n",
    "X_train=pd.read_csv('loan_prediction/X_train.csv')\n",
    "Y_train=pd.read_csv('loan_prediction/Y_train.csv')\n",
    "\n",
    "# test data\n",
    "X_test=pd.read_csv('loan_prediction/X_test.csv')\n",
    "Y_test=pd.read_csv('loan_prediction/Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA VISUALISATION\n",
    "\n",
    "The first step I would follow is visualize the given data before we proceed. This step will give you an idea behind the data distribution which gives us an insight to whether we have any outliners(noisy data),data scale of the given feature and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X_train[X_train.dtypes[(X_train.dtypes==\"float64\")|(X_train.dtypes==\"int64\")]\n",
    "                        .index.values].hist(figsize=[11,11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.PairGrid(X_train)\n",
    "g.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we do not have any missing values but in real time we might have missing values and we can use the following code in which we fill the missing values with the mean value of that feature\n",
    "<br>\n",
    "<br>\n",
    "X_train = X_train.fillna({\"ApplicantIncome\": X_train[\"ApplicantIncome\"].mean(), \"LoanAmount\": X_train[\"LoanAmount\"].mean()\n",
    "                     })\n",
    "<br>\n",
    "<br>\n",
    "Instead of replacing with mean we can also replace with median which behinds on the distribution of data\n",
    "\n",
    "### Remove Outliners\n",
    "\n",
    "X_train = X_train.drop(X_train[X_train.ApplicantIncome > 50000].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']],Y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,knn.predict(X_test[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                             'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got around 61% of correct prediction which is not bad but in real world practices will this be enough ? Can we deploy this model in real world problem? To answer this question lets take a look at distribution of Loan_Status in train data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.Target.value_counts()/Y_train.Target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.Target.value_counts()/Y_test.Target.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max=MinMaxScaler()\n",
    "\n",
    "X_train_minmax=min_max.fit_transform(X_train[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
    "X_test_minmax=min_max.fit_transform(X_test[['ApplicantIncome', 'CoapplicantIncome',\n",
    "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_minmax,Y_train)\n",
    "# Checking the model's accuracy\n",
    "accuracy_score(Y_test,knn.predict(X_test_minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections, we did the pre-processing for continuous numeric features. But, our data set has other features too such as Gender, Married, Dependents, Self_Employed and Education. All these categorical features have string values. For example, Gender has two levels either Male or Female. Lets feed the features in our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fitting a logistic regression model on whole data\n",
    "log=LogisticRegression(penalty='l2',C=.01)\n",
    "log.fit(X_train,Y_train)\n",
    "# Checking the model's accuracy\n",
    "accuracy_score(Y_test,log.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "# Iterating over all the common columns in train and test\n",
    "for col in X_test.columns.values:\n",
    "    # Encoding only categorical variables\n",
    "    if X_test[col].dtypes=='object':\n",
    "        # Using whole data to form an exhaustive list of levels\n",
    "        data=X_train[col].append(X_test[col])\n",
    "        le.fit(data.values)\n",
    "        X_train[col]=le.transform(X_train[col])\n",
    "        X_test[col]=le.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "# Standardizing the features\n",
    "X_train_scale=scale(X_train)\n",
    "X_test_scale=scale(X_test)\n",
    "# Fitting the logistic regression model\n",
    "log=LogisticRegression(penalty='l2',C=.01)\n",
    "log.fit(X_train_scale,Y_train)\n",
    "# Checking the models accuracy\n",
    "accuracy_score(Y_test,log.predict(X_test_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its working now. But, the accuracy is still the same as we got with logistic regression after standardization from numeric features. This means categorical features we added are not very significant in our objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we increase the accuracy ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess your answer in live chat!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
